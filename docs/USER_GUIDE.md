# User Guide

## Table of Contents
1. [Getting Started](#getting-started)
2. [Dataset Management](#dataset-management)
3. [Data Processing Pipeline](#data-processing-pipeline)
4. [Feature Engineering](#feature-engineering)
5. [Model Training](#model-training)
6. [Model Evaluation](#model-evaluation)
7. [Making Predictions](#making-predictions)
8. [Advanced Features](#advanced-features)
9. [Troubleshooting](#troubleshooting)
10. [Best Practices](#best-practices)

## Getting Started

### First Launch
1. **Start the application:**
   ```bash
   python main.py
   ```

2. **Access the web interface:**
   Open your browser and navigate to `http://localhost:5000`

3. **Main Dashboard:**
   The dashboard provides access to all system features:
   - Dataset upload and management
   - Data processing pipeline
   - Feature extraction
   - Model training and evaluation
   - Predictions and analysis

### Interface Overview
The system uses a modern, intuitive web interface with:
- **Navigation Bar**: Quick access to main features
- **Dashboard**: Overview of current datasets and models
- **Progress Indicators**: Real-time status updates
- **AI Insights**: Intelligent recommendations at each step

## Dataset Management

### Supported Data Formats
- **CSV files** (.csv)
- **Excel files** (.xlsx)

### Required Data Structure
Your dataset should contain at minimum:
```csv
text,LABEL
"This is a sample tweet",0
"Another tweet example",1
```

#### Essential Columns
- **text**: Tweet content (REQUIRED) - Accepts variations: 'text', 'TWEET', 'TWEET_CONTENT', 'tweet_content', 'COMBINED_TEXT'
- **LABEL**: Classification label (REQUIRED) - 0 = legitimate, 1 = misinformation

#### Optional Columns (for behavioral features)
- **retweet_count**: Number of retweets
- **favorite_count**: Number of likes/favorites  
- **reply_count**: Number of replies
- **user_id**: User identifier
- **mentions_in_tweet**: Number of mentions
- **hashtags**: Number of hashtags
- **urls_in_tweet**: Number of URLs

**Note**: User-level columns (followers_count, account_age, verification status) are not available in tweet-level datasets. Behavioral features focus on engagement metrics available at the tweet level.

### Uploading Datasets

#### Step 1: Navigate to Upload Page
- Click "Upload Dataset" from the main dashboard
- Or go directly to `/upload`

#### Step 2: Select Your File
- Click "Choose File" and select your CSV or Excel file
- Optionally provide a custom dataset name
- If no name is provided, a timestamp-based name will be generated

#### Step 3: Automatic Processing
The system will automatically:
- Validate the file format
- Check for required columns
- Perform initial data cleaning
- Generate dataset statistics
- Provide AI-powered insights

#### Step 4: Review Dataset Overview
After upload, you'll see:
- **Dataset Statistics**: Sample count, features, missing values
- **Data Quality Assessment**: Balance, completeness indicators
- **AI Insights**: Recommendations for next steps
- **Column Mapping**: How your columns were interpreted

### Dataset Examples

#### Basic Dataset
```csv
text,LABEL
"Breaking: Major earthquake hits city",0
"FAKE NEWS: Aliens landed yesterday!!!",1
"Weather forecast shows rain tomorrow",0
"URGENT: Government hiding the truth!",1
```

#### Enhanced Dataset
```csv
text,LABEL,user_id,retweet_count,favorite_count,user_followers_count
"Breaking news update",0,user123,5,10,1000
"Suspicious claim here",1,user456,50,100,50
"Normal weather update",0,user789,2,5,500
```

## Data Processing Pipeline

### Automatic Processing Steps

#### 1. Data Validation
- File format verification
- Column presence checking
- Data type validation
- Encoding detection and correction

#### 2. Data Cleaning
- **Text Normalization**: Lowercasing, whitespace removal
- **URL Handling**: URL extraction and normalization
- **Mention Processing**: User mention standardization
- **Hashtag Processing**: Hashtag extraction and cleaning
- **Special Character Handling**: Emoji and symbol processing

#### 3. Missing Value Handling
- **Text Fields**: Empty strings replaced with placeholders
- **Numerical Fields**: Zero-filling or median imputation
- **Categorical Fields**: Mode imputation or "unknown" category

#### 4. Data Validation
- **Duplicate Detection**: Identification and handling of duplicates
- **Outlier Detection**: Statistical outlier identification
- **Quality Scoring**: Overall data quality assessment

### Processing Configuration
You can customize processing behavior in the dataset overview:
- **Text Preprocessing Level**: Basic, Standard, or Aggressive
- **Missing Value Strategy**: Drop, Impute, or Flag
- **Duplicate Handling**: Remove, Keep First, or Mark
- **Language Detection**: Enable/disable automatic language detection

## Feature Engineering

### Feature Types

#### 1. Text Features

##### TF-IDF Features
- **Purpose**: Capture important terms and phrases
- **Configuration**: 
  - Vocabulary size: 1000-10000 terms
  - N-gram range: 1-3 grams
  - Stop word removal: Enabled by default

##### Sentiment Features
- **Sentiment Polarity**: Positive, negative, neutral scores
- **Emotion Detection**: Joy, anger, fear, sadness, surprise, disgust
- **Subjectivity**: Objective vs. subjective content
- **Intensity**: Emotional intensity scoring

##### Linguistic Features
- **Readability Metrics**: Flesch-Kincaid, SMOG, ARI scores
- **Complexity Measures**: Sentence length, word complexity
- **Syntactic Features**: POS tag distributions
- **Lexical Diversity**: Type-token ratio, vocabulary richness

##### Transformer Embeddings
- **BERT Embeddings**: Contextual word representations
- **Sentence Transformers**: Semantic sentence embeddings
- **Custom Models**: Domain-specific transformer models

#### 2. Behavioral Features

##### Engagement Metrics (Tweet-Level)
- **Retweet Count**: Number of retweets for the tweet
- **Like/Favorite Count**: Number of favorites/likes
- **Reply Count**: Number of replies to the tweet
- **URL Count**: Number of URLs in tweet
- **Mention Count**: Number of user mentions
- **Hashtag Count**: Number of hashtags

##### Available User Profile Features
- **User ID**: User identifier
- **User Mentions**: How often user is mentioned

##### Note on User-Level Features
The following user-level features are NOT available in tweet-level datasets:
- Account age, follower count, following count, verification status
- These features would require user-level data in your dataset
- Behavioral analysis focuses on tweet engagement patterns instead

##### Temporal Features
- **Posting Time**: Hour of day patterns
- **Recency**: Time since posting
- **Engagement Rate**: Engagement relative to time posted

#### 3. Network Features

##### Centrality Measures
- **Degree Centrality**: Direct connection count
- **Betweenness Centrality**: Bridge position in network
- **Closeness Centrality**: Average distance to other nodes
- **Eigenvector Centrality**: Influence-weighted connections

##### Community Features
- **Community Membership**: Cluster assignment
- **Community Size**: Size of user's community
- **Inter-community Connections**: Cross-cluster relationships
- **Community Cohesion**: Internal connection density

##### Propagation Features
- **Retweet Chains**: Length and structure of retweet paths
- **Information Cascade**: Cascade size and depth
- **Influence Spread**: Reach and penetration metrics
- **Echo Chamber Detection**: Homophily measures

### Feature Extraction Process

#### Step 1: Select Feature Types
Navigate to the Feature Extraction page and choose:
- **Text Features**: Enable/disable specific text feature types
- **Behavioral Features**: Select engagement and user features
- **Network Features**: Choose network analysis features
- **Theoretical Features**: RAT and RCT framework features

#### Step 2: Configure Parameters
- **Text Processing**: Language, preprocessing level
- **Embedding Models**: Choose transformer models
- **Network Analysis**: Community detection algorithms
- **Feature Selection**: Automatic feature selection methods

#### Step 3: Monitor Extraction
The system provides real-time progress updates:
- **Current Stage**: Which features are being extracted
- **Progress Bar**: Completion percentage
- **Time Estimates**: Remaining processing time
- **Resource Usage**: Memory and CPU utilization

#### Step 4: Review Results
After extraction, review:
- **Feature Summary**: Count and types of extracted features
- **Feature Importance**: Preliminary importance scores
- **Quality Metrics**: Feature quality indicators
- **AI Recommendations**: Suggestions for optimization

### Theoretical Framework Features

#### Routine Activity Theory (RAT) Features

##### Motivated Offenders
- **User Anonymity**: Account anonymity indicators
- **Behavioral Patterns**: Suspicious activity patterns
- **Content Characteristics**: Deceptive content markers
- **Network Position**: Structural positions favoring deception

##### Suitable Targets
- **Content Vulnerability**: Susceptibility to manipulation
- **Audience Characteristics**: Target audience analysis
- **Topic Sensitivity**: Controversial topic indicators
- **Emotional Triggers**: Emotional manipulation potential

##### Capable Guardians
- **Fact-checking Presence**: Fact-checker engagement
- **Expert Involvement**: Domain expert participation
- **Platform Moderation**: Moderation signal strength
- **Community Policing**: Peer correction mechanisms

#### Rational Choice Theory (RCT) Features

##### Cost-Benefit Analysis
- **Deception Costs**: Potential consequences of misinformation
- **Reward Potential**: Possible benefits from spreading false information
- **Detection Risk**: Likelihood of being caught
- **Social Costs**: Reputation and relationship impacts

##### Decision-Making Context
- **Information Environment**: Information availability and quality
- **Social Pressure**: Peer influence and social norms
- **Cognitive Load**: Mental effort required for verification
- **Time Constraints**: Urgency and time pressure factors

## Model Training

### Supported Algorithms

#### 1. Logistic Regression
- **Best For**: Linear relationships, interpretable results
- **Advantages**: Fast training, good baseline performance
- **Parameters**: Regularization strength, solver type
- **Use Cases**: Initial exploration, feature importance analysis

#### 2. Naive Bayes
- **Best For**: Text classification, small datasets
- **Advantages**: Fast training, works well with limited data
- **Parameters**: Smoothing parameter, feature selection
- **Use Cases**: Quick prototyping, text-heavy datasets

#### 3. Random Forest
- **Best For**: Mixed feature types, robust performance
- **Advantages**: Handles overfitting well, feature importance
- **Parameters**: Number of trees, max depth, min samples
- **Use Cases**: General-purpose classification, feature selection

#### 4. Gradient Boosting
- **Best For**: High performance, complex patterns
- **Advantages**: Often best performance, handles missing values
- **Parameters**: Learning rate, number of estimators, max depth
- **Use Cases**: Competition-level performance, complex datasets

#### 5. Neural Networks
- **Best For**: Large datasets, complex patterns
- **Advantages**: Can learn complex non-linear relationships
- **Parameters**: Hidden layers, neurons, activation functions
- **Use Cases**: Large-scale datasets, deep learning applications

#### 6. Support Vector Machine (SVM)
- **Best For**: High-dimensional data, binary classification
- **Advantages**: Effective in high-dimensional spaces, memory efficient
- **Parameters**: Regularization (C), kernel type, gamma
- **Use Cases**: Text classification, complex decision boundaries

### Hyperparameter Tuning and Optimization

Hyperparameter tuning is the process of finding optimal parameter values for your machine learning models to maximize performance. The system provides both manual and automated tuning capabilities.

#### Available Tuning Methods

**Grid Search CV (Cross-Validation)**
- **Strategy**: Exhaustively searches all parameter combinations
- **Advantages**: Guaranteed to find optimal parameters within search space
- **Disadvantages**: Computationally expensive for large parameter spaces
- **Best For**: Smaller parameter spaces, final optimization
- **Default Cross-Validation**: 5-fold cross-validation

**Random Search CV**
- **Strategy**: Randomly samples parameter combinations
- **Advantages**: More efficient for large parameter spaces
- **Disadvantages**: May miss optimal parameters
- **Best For**: Large parameter spaces, initial exploration
- **Default Iterations**: 50 random samples

#### Algorithm-Specific Hyperparameters

**Logistic Regression Parameters**
```python
{
    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength (inverse)
    'penalty': ['l1', 'l2'],        # Regularization type
    'solver': ['liblinear', 'saga'], # Optimization algorithm
    'max_iter': [1000, 2000]        # Maximum iterations
}
```
- **C**: Controls model complexity (smaller = more regularization)
- **penalty**: L1 (sparse) or L2 (dense) regularization
- **solver**: liblinear for small datasets, saga for large
- **max_iter**: Increase if solver doesn't converge

**Random Forest Parameters**
```python
{
    'n_estimators': [50, 100, 200],    # Number of trees
    'max_depth': [None, 10, 20, 30],   # Tree depth (None = unlimited)
    'min_samples_split': [2, 5, 10],   # Samples required to split
    'min_samples_leaf': [1, 2, 4],     # Samples required at leaf
    'max_features': ['sqrt', 'log2']   # Features to consider per split
}
```
- **n_estimators**: More trees = better but slower
- **max_depth**: Limit depth to prevent overfitting
- **min_samples_split/leaf**: Control tree growth and overfitting

**Gradient Boosting Parameters**
```python
{
    'n_estimators': [50, 100, 200],    # Number of boosting stages
    'learning_rate': [0.01, 0.1, 0.2], # Shrinkage parameter
    'max_depth': [3, 5, 7],             # Tree depth
    'min_samples_split': [2, 5, 10],    # Samples for split
    'subsample': [0.8, 0.9, 1.0]       # Fraction of samples per iteration
}
```
- **learning_rate**: Lower values = slower but potentially better
- **n_estimators**: Stop early if validation performance plateaus
- **max_depth**: Shallow trees work better in boosting
- **subsample**: Stochastic gradient boosting for regularization

**Support Vector Machine (SVM) Parameters**
```python
{
    'C': [0.1, 1, 10, 100],           # Regularization strength
    'kernel': ['linear', 'rbf', 'poly'], # Kernel type
    'gamma': ['scale', 'auto', 0.001, 0.01] # Kernel coefficient
}
```
- **C**: Smaller = more regularization, larger = fit training data more
- **kernel**: linear (fast, good for text), rbf (flexible), poly (specific patterns)
- **gamma**: Controls influence of single training sample (for rbf/poly)

**Naive Bayes Parameters**
```python
{
    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6] # Laplace smoothing
}
```
- **var_smoothing**: Small values = less smoothing (fits data more closely)

**Neural Network Parameters**
```python
{
    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)], # Hidden layers
    'activation': ['relu', 'tanh', 'logistic'],                   # Activation function
    'alpha': [0.0001, 0.001, 0.01],                              # L2 regularization
    'learning_rate': ['constant', 'adaptive'],                    # Learning rate type
    'max_iter': [500, 1000]                                       # Max iterations
}
```
- **hidden_layer_sizes**: Tuple of layer sizes (50,) = 1 layer with 50 neurons
- **activation**: relu (recommended), tanh (alternative), logistic (sigmoid)
- **alpha**: L2 regularization strength
- **learning_rate**: constant (fixed rate) or adaptive (decreases over time)

#### Tuning Process in the Web Interface

**Step 1: Access Hyperparameter Configuration**
1. Go to "Model Training" â†’ "Advanced Options"
2. Click "Configure Hyperparameters"
3. Choose optimization method (Grid Search or Random Search)

**Step 2: Select Models and Methods**
- Check models to tune: Logistic Regression, Random Forest, etc.
- Choose tuning method: Grid Search (thorough) or Random Search (faster)
- Set cross-validation folds: 3-5 (default: 5)
- Select scoring metric: F1-Score (recommended for imbalanced data), Accuracy

**Step 3: Configure Parameter Ranges**
For each model:
- **Logistic Regression**: Set C values, penalties
- **Random Forest**: Configure n_estimators, max_depth
- **Gradient Boosting**: Set learning rate, estimators
- Use defaults or enter custom ranges

**Step 4: Monitor Optimization**
- Real-time progress of parameter search
- Current best parameters and score
- Estimated time remaining
- Option to stop early if satisfied

**Step 5: Review Results**
- **Best Parameters**: Optimal values found
- **Best Score**: Cross-validation performance
- **Performance Comparison**: All tested combinations
- **Recommendation**: System's best model suggestion

#### Manual Hyperparameter Tuning Strategy

**For Logistic Regression:**
1. Start with C=1.0, penalty='l2'
2. If overfitting: decrease C (more regularization)
3. If underfitting: increase C (less regularization)
4. Try both 'l1' and 'l2' penalties

**For Random Forest:**
1. Start with n_estimators=100, max_depth=None
2. If overfitting: decrease max_depth or increase min_samples_leaf
3. If underfitting: increase n_estimators
4. Tune max_features: 'sqrt' for many features, 'log2' for fewer

**For Gradient Boosting:**
1. Start with learning_rate=0.1, n_estimators=100
2. If overfitting: decrease learning_rate
3. If underfitting: increase n_estimators or learning_rate
4. Always use smaller learning_rate with more estimators

**General Strategy:**
1. **Baseline**: Train with default parameters
2. **Coarse Search**: Test wide parameter ranges
3. **Fine Search**: Focus on promising regions
4. **Validation**: Test on hold-out validation set
5. **Stopping Rule**: Stop when performance plateaus

#### Recommended Parameter Combinations

**Best All-Around (Balanced Performance/Speed):**
```python
{
    'algorithm': 'random_forest',
    'n_estimators': 100,
    'max_depth': 15,
    'min_samples_leaf': 2,
    'max_features': 'sqrt'
}
```

**Best Performance (Highest Accuracy):**
```python
{
    'algorithm': 'gradient_boosting',
    'n_estimators': 200,
    'learning_rate': 0.1,
    'max_depth': 5,
    'subsample': 0.8
}
```

**Best Speed (Fastest Training):**
```python
{
    'algorithm': 'logistic_regression',
    'C': 1.0,
    'penalty': 'l2',
    'solver': 'liblinear'
}
```

**Best for Text Data (Large Feature Count):**
```python
{
    'algorithm': 'svm',
    'C': 1.0,
    'kernel': 'linear',
    'gamma': 'scale'
}
```

### Training Process

#### Step 1: Select Algorithms
- Choose one or more algorithms to train
- Consider dataset size and computational resources
- Review AI recommendations for algorithm selection

#### Step 2: Configure Training Parameters
- **Train-Test Split**: Typically 80-20 or 70-30
- **Cross-Validation**: K-fold validation (default: 5-fold)
- **Class Balancing**: Handle imbalanced datasets
- **Feature Selection**: Automatic feature selection options

#### Step 3: Monitor Training
Real-time monitoring includes:
- **Training Progress**: Current algorithm and progress
- **Performance Metrics**: Live accuracy and loss updates
- **Resource Usage**: CPU, memory, and time consumption
- **Early Stopping**: Automatic stopping for optimal performance

#### Step 4: Review Results
After training, examine:
- **Performance Comparison**: Side-by-side algorithm comparison
- **Best Model Selection**: Automatic best model identification
- **Training Logs**: Detailed training history
- **Model Artifacts**: Saved models and metadata

### Training Configuration

#### Basic Configuration
```json
{
  "algorithms": ["logistic_regression", "random_forest", "gradient_boosting"],
  "test_size": 0.2,
  "cross_validation": true,
  "cv_folds": 5,
  "random_state": 42
}
```

#### Advanced Configuration
```json
{
  "algorithms": ["random_forest", "gradient_boosting"],
  "hyperparameter_tuning": true,
  "tuning_method": "grid_search",
  "scoring_metric": "f1_weighted",
  "class_balancing": "smote",
  "feature_selection": "recursive_elimination"
}
```

## Model Evaluation

### Performance Metrics

#### Classification Metrics
- **Accuracy**: Overall correctness percentage
- **Precision**: True positives / (True positives + False positives)
- **Recall**: True positives / (True positives + False negatives)
- **F1-Score**: Harmonic mean of precision and recall
- **ROC-AUC**: Area under the receiver operating characteristic curve

#### Class-Specific Metrics
- **Per-Class Precision**: Precision for each class
- **Per-Class Recall**: Recall for each class
- **Per-Class F1-Score**: F1-score for each class
- **Support**: Number of samples per class

#### Advanced Metrics
- **Matthews Correlation Coefficient**: Balanced measure for imbalanced datasets
- **Cohen's Kappa**: Agreement measure accounting for chance
- **Log Loss**: Probabilistic loss function
- **Brier Score**: Calibration quality measure

### Visualization Tools

#### Confusion Matrix
- **Purpose**: Detailed error analysis
- **Interpretation**: True vs. predicted class distribution
- **Features**: Normalized and raw count versions
- **Insights**: Identify specific misclassification patterns

#### ROC Curves
- **Purpose**: Threshold-independent performance assessment
- **Interpretation**: True positive rate vs. false positive rate
- **Features**: Multi-class ROC curves, AUC scores
- **Insights**: Compare model discrimination ability

#### Precision-Recall Curves
- **Purpose**: Performance on imbalanced datasets
- **Interpretation**: Precision vs. recall trade-offs
- **Features**: Average precision scores
- **Insights**: Optimal threshold selection

#### Feature Importance
- **Purpose**: Understand model decision factors
- **Interpretation**: Relative importance of input features
- **Features**: Multiple importance measures
- **Insights**: Feature selection and model interpretation

#### Learning Curves
- **Purpose**: Assess model learning behavior
- **Interpretation**: Performance vs. training set size
- **Features**: Training and validation curves
- **Insights**: Detect overfitting and underfitting

### Model Comparison

#### Performance Comparison Table
| Model | Accuracy | F1-Score | ROC-AUC | Training Time |
|-------|----------|----------|---------|---------------|
| Logistic Regression | 0.87 | 0.85 | 0.91 | 2.3s |
| Random Forest | 0.92 | 0.90 | 0.94 | 15.7s |
| Gradient Boosting | 0.94 | 0.92 | 0.96 | 45.2s |

#### Model Selection Criteria
- **Performance**: Primary metric optimization
- **Speed**: Training and prediction time
- **Interpretability**: Model explainability requirements
- **Robustness**: Performance stability across datasets
- **Resource Usage**: Memory and computational requirements

## Making Predictions

### Single Prediction
1. **Navigate to Prediction Page**: Click "Make Predictions"
2. **Enter Text**: Type or paste the tweet text
3. **Select Model**: Choose from trained models
4. **Get Results**: View prediction with confidence score
5. **Explanation**: Optional SHAP-based explanation

### Batch Predictions
1. **Upload File**: CSV file with text column
2. **Configure Settings**: Select model and output format
3. **Process**: Automatic batch processing
4. **Download Results**: CSV file with predictions and probabilities

### Live Predictions
- **Real-time Processing**: Instant predictions as you type
- **Confidence Scoring**: Probability estimates
- **Feature Highlighting**: Important words and phrases
- **Explanation**: Why the model made this prediction

### Prediction Output Format
```json
{
  "prediction": {
    "class": 1,
    "probability": 0.78,
    "confidence": "high"
  },
  "explanation": {
    "top_features": [
      {"feature": "sentiment_negative", "contribution": 0.25},
      {"feature": "tfidf_fake", "contribution": 0.18},
      {"feature": "exclamation_count", "contribution": 0.12}
    ]
  },
  "metadata": {
    "model_used": "random_forest",
    "processing_time": 0.15,
    "feature_count": 540
  }
}
```

## Advanced Features

### Zero-Shot Classification
For datasets without labels:

#### Step 1: Define Labels
- Create custom label definitions
- Example: ["misinformation", "legitimate", "satire", "opinion"]

#### Step 2: Configure Classification
- **Hypothesis Template**: "This text is {}"
- **Confidence Threshold**: Minimum confidence for classification
- **Batch Size**: Processing batch size for efficiency

#### Step 3: Review Results
- **Label Distribution**: Proportion of each label
- **Confidence Scores**: Average confidence per label
- **Quality Assessment**: Classification quality indicators

### Language Detection
Automatic language identification:

#### Supported Languages
- English, Spanish, French, German, Italian, Portuguese
- Arabic, Chinese, Japanese, Korean, Russian
- And 50+ additional languages

#### Features
- **Language Distribution**: Percentage of each language
- **Multilingual Detection**: Mixed-language content identification
- **Confidence Scoring**: Language detection confidence
- **Language-Specific Processing**: Tailored processing per language

### Sentiment Analysis
Comprehensive emotional analysis:

#### Sentiment Dimensions
- **Polarity**: Positive, negative, neutral
- **Subjectivity**: Objective vs. subjective
- **Intensity**: Emotional strength
- **Emotions**: Joy, anger, fear, sadness, surprise, disgust

#### Applications
- **Content Filtering**: Filter by emotional content
- **Trend Analysis**: Emotional trends over time
- **User Profiling**: Emotional characteristics of users
- **Misinformation Correlation**: Emotion-misinformation relationships

### Network Analysis
Social network analysis capabilities:

#### Network Metrics
- **Centrality Measures**: Identify influential users
- **Community Detection**: Find user groups
- **Clustering Coefficient**: Network cohesion
- **Path Length**: Information flow efficiency

#### Visualizations
- **Network Graphs**: Interactive network visualizations
- **Community Maps**: Color-coded community structures
- **Influence Heatmaps**: Influence distribution maps
- **Temporal Networks**: Network evolution over time

### Explainable AI
Model interpretation and explanation:

#### SHAP Analysis
- **Global Explanations**: Overall feature importance
- **Local Explanations**: Individual prediction explanations
- **Partial Dependence**: Feature effect visualization
- **Interaction Effects**: Feature interaction analysis

#### Feature Importance
- **Permutation Importance**: Model-agnostic importance
- **Tree-based Importance**: For tree-based models
- **Coefficient Analysis**: For linear models
- **Attention Weights**: For neural network models

## Troubleshooting

### Common Issues

#### 1. Dataset Upload Problems
**Issue**: File upload fails
**Solutions**:
- Check file format (CSV or Excel only)
- Verify file size (max 16MB)
- Ensure required columns are present
- Check for special characters in filename

#### 2. Feature Extraction Errors
**Issue**: Feature extraction fails or takes too long
**Solutions**:
- Reduce dataset size for testing
- Check available memory
- Verify internet connection (for transformer models)
- Try different feature combinations

#### 3. Model Training Issues
**Issue**: Training fails or produces poor results
**Solutions**:
- Check data quality and balance
- Try different algorithms
- Adjust hyperparameters
- Increase training data size

#### 4. Prediction Errors
**Issue**: Predictions fail or seem incorrect
**Solutions**:
- Verify model is trained
- Check input text format
- Review feature extraction status
- Compare with training data format

#### 5. Performance Issues
**Issue**: System runs slowly
**Solutions**:
- Close unnecessary applications
- Reduce batch sizes
- Use smaller models
- Enable GPU acceleration (if available)

### Error Messages

#### "Dataset not found"
- **Cause**: Dataset was deleted or moved
- **Solution**: Re-upload the dataset

#### "Model not trained"
- **Cause**: Attempting predictions without training
- **Solution**: Train models first

#### "Insufficient memory"
- **Cause**: Not enough RAM for processing
- **Solution**: Reduce dataset size or increase system memory

#### "Feature extraction failed"
- **Cause**: Error in feature processing
- **Solution**: Check logs, try different feature types

### Getting Help
1. **Check Application Logs**: Look in `logs/app.log`
2. **Review AI Insights**: System provides intelligent recommendations
3. **Use Diagnostic Tools**: Built-in system diagnostics
4. **Check System Status**: Verify all components are working

## Best Practices

### Data Preparation
1. **Clean Your Data**: Remove duplicates, handle missing values
2. **Balance Your Dataset**: Ensure reasonable class distribution
3. **Quality Over Quantity**: Better to have clean, smaller datasets
4. **Validate Labels**: Ensure label accuracy and consistency

### Feature Engineering
1. **Start Simple**: Begin with basic features, add complexity gradually
2. **Domain Knowledge**: Incorporate domain-specific features
3. **Feature Selection**: Remove irrelevant or redundant features
4. **Scaling**: Ensure features are on similar scales

### Model Training
1. **Baseline First**: Start with simple models as baselines
2. **Cross-Validation**: Always use cross-validation for reliable estimates
3. **Hyperparameter Tuning**: Optimize model parameters systematically
4. **Ensemble Methods**: Combine multiple models for better performance

### Evaluation
1. **Multiple Metrics**: Don't rely on accuracy alone
2. **Class-Specific Analysis**: Examine per-class performance
3. **Error Analysis**: Understand where models fail
4. **Validation Strategy**: Use appropriate validation techniques

### Production Use
1. **Monitor Performance**: Track model performance over time
2. **Regular Retraining**: Update models with new data
3. **A/B Testing**: Compare model versions systematically
4. **Feedback Loop**: Incorporate user feedback for improvement

### Security and Privacy
1. **Data Protection**: Ensure sensitive data is protected
2. **Model Security**: Protect models from adversarial attacks
3. **Privacy Compliance**: Follow relevant privacy regulations
4. **Access Control**: Implement appropriate access controls

---

**Ready to become a misinformation detection expert? Start with your first dataset!** 